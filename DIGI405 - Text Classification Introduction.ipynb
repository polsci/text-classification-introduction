{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 - Text Classification Introduction\n",
    "\n",
    "This lab notebook will introduce text classification using **[Scikit-learn](https://scikit-learn.org/stable/)**.\n",
    "\n",
    "This is quite a long notebook. It will take you through loading and inspecting the texts in your corpus, setting up feature extraction, classifying your texts and evaluating your text classification model. \n",
    "\n",
    "*Note*: You will need to jump around the notebook to change settings and rerun the classification to find good settings for feature extraction and to evaluate your model. \n",
    "\n",
    "**Important:** Each time you change settings below, you need to rerun the following cells in order to implement the classification pipeline.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Below we are importing required libraries. \n",
    "\n",
    "We will use the [Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html). We will also use Scikit-learn's different feature extraction methods based on counts or tf-idf weights. The [NLTK](https://www.nltk.org/) library is used for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step downloads the following [NLTK](https://www.nltk.org/) resources: stopwords, the POS tagger (used by the NLTK lemmatizer), the Punkt tokenizer models, and the [WordNet lexical database](https://wordnet.princeton.edu/) (used for word meanings and relationships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads some defaults for the stop word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = None\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions (you don't need to change anything here, just run the cell) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice preview of document\n",
    "def get_preview(docs, targets, target_names, doc_id, max_len=0):\n",
    "    preview = ''\n",
    "    if max_len < 1:\n",
    "        preview += 'Label\\n'\n",
    "        preview += '=====\\n'\n",
    "    else:\n",
    "        preview += str(doc_id)\n",
    "        preview += '\\t'\n",
    "    preview += target_names[targets[doc_id]]\n",
    "    if max_len < 1:\n",
    "        preview += '\\n\\nFull Text\\n'\n",
    "        preview += '=========\\n'\n",
    "        preview += docs[doc_id]\n",
    "        preview += '\\n'\n",
    "    else:\n",
    "        excerpt = get_excerpt(docs[doc_id], max_len)\n",
    "        preview += '\\t' + excerpt\n",
    "    return preview\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "# generate an excerpt\n",
    "def get_excerpt(text, max_len):\n",
    "    excerpt = _RE_COMBINE_WHITESPACE.sub(' ',text[0:max_len])\n",
    "    if max_len < len(text):\n",
    "        excerpt += '...'\n",
    "    return excerpt.strip()\n",
    "\n",
    "# combine a defined stop word list (or no stop word list) with any extra stop words defined\n",
    "def set_stop_words(stop_word_list, extra_stop_words):\n",
    "    if len(extra_stop_words) > 0:\n",
    "        if stop_word_list is None:\n",
    "            stop_word_list = []\n",
    "        stop_words = list(stop_word_list) + extra_stop_words\n",
    "    else:\n",
    "        stop_words = stop_word_list\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "# initiate stemming or lemmatising\n",
    "def set_normaliser(normalise):\n",
    "    if normalise == 'PorterStemmer':\n",
    "        normaliser = PorterStemmer()\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        normaliser = SnowballStemmer('english')\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        normaliser = WordNetLemmatizer()\n",
    "    else:\n",
    "        normaliser = None\n",
    "    return normaliser\n",
    "\n",
    "# we are using a custom tokenisation process to allow different tokenisers and stemming/lemmatising ...\n",
    "def tokenise(doc):\n",
    "    global tokeniser, normalise, normaliser\n",
    "    \n",
    "    # you could obviously add more tokenisers here if you wanted ...\n",
    "    if tokeniser == 'sklearn':\n",
    "        tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\") # this is copied straight from sklearn source\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "    elif tokeniser == 'word_tokenize':\n",
    "        tokens = word_tokenize(doc)\n",
    "    elif tokeniser == 'wordpunct':\n",
    "        tokens = wordpunct_tokenize(doc)\n",
    "    else:\n",
    "        tokens = word_tokenize(doc)\n",
    "        \n",
    "    # if using a normaliser then iterate through tokens and return the normalised tokens ...\n",
    "    if normalise == 'PorterStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'SnowballStemmer':\n",
    "        return [normaliser.stem(t) for t in tokens]\n",
    "    elif normalise == 'WordNetLemmatizer':\n",
    "        # NLTK's lemmatiser needs parts of speech, otherwise assumes everything is a noun\n",
    "        pos_tokens = nltk.pos_tag(tokens)\n",
    "        lemmatised_tokens = []\n",
    "        for token in pos_tokens:\n",
    "            # NLTK's lemmatiser needs specific values for pos tags - this rewrites them ...\n",
    "            # default to noun\n",
    "            tag = wordnet.NOUN\n",
    "            if token[1].startswith('J'):\n",
    "                tag = wordnet.ADJ\n",
    "            elif token[1].startswith('V'):\n",
    "                tag = wordnet.VERB\n",
    "            elif token[1].startswith('R'):\n",
    "                tag = wordnet.ADV\n",
    "            lemmatised_tokens.append(normaliser.lemmatize(token[0],tag))\n",
    "        return lemmatised_tokens\n",
    "    else:\n",
    "        # no normaliser so just return tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview stop word lists\n",
    "\n",
    "As discussed in the lecture material, pre-processing can have a major influence on the results of text classification tasks. \n",
    "\n",
    "In particular, you should put thought into whether a stop word list is sensible for your task. The scikit-learn website also makes this point at https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words and recommends caution about using its stop word list! That page also links to a recent paper discussing practical issues with stop word lists, including whether the way you are tokenising your documents matches the tokenisation approach used in your stop word list.\n",
    "\n",
    "Using the cells below you can preview the stop word lists supplied by scikit-learn and NLTK, which we have used previously in class. You will notice they are quite different.\n",
    "\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> For each stop word list, think of a text classification task where words in the stop word list could be informative and where it would be a bad idea to remove them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus and set train/test split\n",
    "\n",
    "Scikit-learn is packaged with a number of standard data-sets used in machine learning and provides a way to load other data. \n",
    "\n",
    "We will begin by loading texts from two categories in the **[20 newsgroups dataset](http://qwone.com/~jason/20Newsgroups/)** to work through an example classifying documents related to politics and religion.\n",
    "\n",
    "*What is a newsgroup?* We are stretching back into internet history here - way before people talked to strangers on Facebook and X and other social media, there were Usenet Newsgroups! [Here is a link to a Deja News page from 1998](https://web.archive.org/web/19980127204536/http://emarket.dejanews.com/emarket/about/idgs/aboutidgs.shtml) and also a [Wikipedia article](https://en.wikipedia.org/wiki/Usenet_newsgroup) that explains what Newsgroups are all about. \n",
    "\n",
    "This data-set was built from discussions between real people on the internet in the 1990s. Please be aware that within this data-set are texts that include racist, sexist, and other offensive language use. \n",
    "\n",
    "**Note:** This cell also sets the following train/test split: **80% of the data is used for training and 20% is used for testing.** The documents are assigned to each group randomly. It can be useful to rerun this cell to reshuffle your dataset so you can evaluate your model using different data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chooses the categories to load\n",
    "cats = ['talk.politics.misc', 'talk.religion.misc']\n",
    "\n",
    "# this downloads/loads the data\n",
    "dataset = fetch_20newsgroups(subset='train', categories=cats)\n",
    "#dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "\n",
    "# assign the train/test split - 0.2 is 80% for training, 20% for testing\n",
    "test_size = 0.2\n",
    "\n",
    "# do the train test split ...\n",
    "# docs_train and docs_test are the documents\n",
    "# y_train and y_test are the labels\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, \n",
    "                                                          test_size = test_size, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect documents and labels\n",
    "\n",
    "In the next cells we can look at the data we have imported. Firstly, we will preview the document labels and a brief excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_id in range(len(docs_train)):\n",
    "    print(get_preview(docs_train, y_train, dataset.target_names, train_id, max_len=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use the following cell to inspect a specific document and its label based on its index in the training set. \n",
    "\n",
    "Note: The indexes will change each time you import the data above because of the random train/test split.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Inspect some of the documents in each class and think about the kinds of words that might be useful features in this text classification task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = 11  # Enter the index of the document you want to preview\n",
    "print(get_preview(docs_train, y_train, dataset.target_names, train_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**This next section of the notebook steps you through some key kinds of pre-processing for text classification using Naive Bayes and a bag of words (BoW) model.**\n",
    "\n",
    "On the first run you should read about each setting, but leave the settings as they are. You will come back to this section to tune your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose between token counts or tf-idf weights\n",
    "\n",
    "You can choose to vectorize your text using frequency or tf-idf weights. Valid values are:\n",
    "```\n",
    "Vectorizer = CountVectorizer\n",
    "```\n",
    "or\n",
    "```\n",
    "Vectorizer = TfidfVectorizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer  # Set the vectorization method you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase\n",
    "\n",
    "Setting lowercase to True will transform all document text to lowercase. Setting it to False will not do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set how you are tokenising the text\n",
    "\n",
    "With this notebook you can choose between the following tokenisers.\n",
    "\n",
    "This option duplicates the behaviour of scikit-learn's default tokeniser: \"The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)\". In this notebook we duplicate this behaviour using the NLTK's regular expression tokeniser and this regular expression: `r\"(?u)\\b\\w\\w+\\b\"`.\n",
    "```\n",
    "tokeniser = 'sklearn'\n",
    "```\n",
    "You can use this or specify one of the following tokenisers based on NLTK ...\n",
    "\n",
    "Tokenise based on NLTK's wordpunct_tokenize tokeniser (to include words and punctuation!):\n",
    "```\n",
    "tokeniser = 'wordpunct'\n",
    "```\n",
    "This applies NLTK's word_tokenize tokeniser.\n",
    "```\n",
    "tokeniser = 'word_tokenize'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = 'sklearn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / Lemmatising\n",
    "\n",
    "This allows to use NLTK stemmers or lemmatisers (or not). Valid options are shown below. Look for more information on the NLTK website: https://www.nltk.org/api/nltk.stem.html. Note: that stemming and lemmatising (in particular) will make the preprocessing take longer!\n",
    "\n",
    "```\n",
    "normalise = None\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'PorterStemmer'\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'SnowballStemmer'\n",
    "```\n",
    "or\n",
    "```\n",
    "normalise = 'WordNetLemmatizer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure stop words\n",
    "\n",
    "Hopefully you have read the notes on stop word lists above and previewed the different lists. \n",
    "\n",
    "Do you want to apply a stop_word list? Valid values for stop_words below are:\n",
    "```\n",
    "stop_word_list = None\n",
    "```\n",
    "or\n",
    "```\n",
    "stop_word_list = nltk_stop_words\n",
    "```\n",
    "or\n",
    "```\n",
    "stop_word_list = list(sklearn_stop_words)\n",
    "```\n",
    "\n",
    "**Note:** the sklearn_stop_words list is downloaded as a set. We use list() to convert it to a list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add extra stop words to any of the lists above.\n",
    "For example:\n",
    "```\n",
    "extra_stop_words = ['stopword1','stopword2','stopword3']\n",
    "```\n",
    "If you don't want extra stop words, then the next cell should look like:\n",
    "```\n",
    "extra_stop_words = []\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stop_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter features based on document frequency\n",
    "\n",
    "The following settings allow you to remove features that occur in many documents or in only a few documents.\n",
    "\n",
    "Firstly, `min_df` ignores terms that occur below a minimum proportion of documents. For example, 0.01 would ignore terms that occur in less than 1% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df` allows you to ignore terms above a maximum proportion of documents. For example, 0.95 would ignore terms that occur in more than 95% of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a maximum number of features\n",
    "\n",
    "`max_features` set this to `None` for no limit or set to the maximum number of the most frequent features (e.g setting it to 1000 would use the 1000 most frequent features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams\n",
    "\n",
    "With ngram_range set to (1,1) you will use unigrams as features i.e. each feature will be a token. If you set it to (1,2) you will use unigrams and bigrams. (1,3) will use unigrams, bigrams and trigrams. If you just want bigrams you would use (2,2). Please note: increasing the ngram range from (1,1) will add more time to preprocessing, as there will be more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding options\n",
    "\n",
    "You can change the default encoding here and what to do if you get characters outside your default encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = 'utf-8'\n",
    "decode_error = 'ignore' # what to do if contains characters not of the given encoding - options 'strict', 'ignore', 'replace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the feature extraction and classification pipeline\n",
    "\n",
    "This sets up a Sci-kit learn pipeline for feature extraction and classification. \n",
    "\n",
    "**Important Note 1:** When you change settings above or reload your dataset you should rerun this cell!\n",
    "\n",
    "**Important Note 2:** This cell outputs the settings you used above, which you can cut and paste into a document to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "\n",
    "stop_words = set_stop_words(stop_word_list, extra_stop_words)\n",
    "normaliser = set_normaliser(normalise)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', Vectorizer(\n",
    "            tokenizer = tokenise,\n",
    "            lowercase = lowercase,\n",
    "            min_df = min_df, \n",
    "            max_df = max_df, \n",
    "            max_features = max_features,\n",
    "            stop_words = stop_words, \n",
    "            ngram_range = ngram_range,\n",
    "            encoding = encoding, \n",
    "            decode_error = decode_error)),\n",
    "    ('classifier', MultinomialNB()), #here is where you would specify an alternative classifier\n",
    "])\n",
    "\n",
    "print('Classifier settings')\n",
    "print('===================')\n",
    "print('classifier:', type(pipeline.steps[1][1]).__name__)\n",
    "print('vectorizer:', type(pipeline.steps[0][1]).__name__)\n",
    "print('classes:', dataset.target_names)\n",
    "print('lowercase:', lowercase)\n",
    "print('tokeniser:', tokeniser)\n",
    "print('normalise:', normalise)\n",
    "print('min_df:', min_df)\n",
    "print('max_df:', max_df)\n",
    "print('max_features:', max_features)\n",
    "if stop_word_list == nltk_stop_words:\n",
    "    print('stop_word_list:', 'nltk_stop_words')\n",
    "elif stop_word_list == list(sklearn_stop_words):\n",
    "    print('stop_word_list:', 'sklearn_stop_words')\n",
    "else:\n",
    "    print('stop_word_list:', 'None')\n",
    "print('extra_stop_words:', extra_stop_words)\n",
    "print('ngram_range:', ngram_range)\n",
    "print('encoding:', encoding)\n",
    "print('decode_error:', decode_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier and predict labels on test data\n",
    "\n",
    "This cell does the work of training the classifier and predicting labels on test data. It also outputs evaluation metrics, a confusion matrix and features indicative of each class.\n",
    "\n",
    "**Important Note:** You can cut and paste the model output into a document (with the settings above) to keep track of changes you are making and their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you shouldn't need to change anything in this cell!\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pipeline.fit(docs_train, y_train)\n",
    "y_predicted = pipeline.predict(docs_test)\n",
    "\n",
    "# print report\n",
    "print('\\nEvaluation metrics')\n",
    "print('==================\\n')\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names = dataset.target_names))\n",
    "cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_predicted, labels=[0, 1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dataset.target_names)\n",
    "disp = disp.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "vect = pipeline.steps[0][1]\n",
    "clf = pipeline.steps[1][1]\n",
    "\n",
    "print()\n",
    "\n",
    "logodds=clf.feature_log_prob_[1]-clf.feature_log_prob_[0]\n",
    "\n",
    "print(\"Features most indicative of\",dataset.target_names[0])\n",
    "print('============================' + '='*len(dataset.target_names[0]))\n",
    "for i in np.argsort(logodds)[:20]:\n",
    "    print(vect.get_feature_names_out()[i], end=' ')\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Features most indicative of\",dataset.target_names[1])\n",
    "print('============================' + '='*len(dataset.target_names[1]))\n",
    "for i in np.argsort(-logodds)[:20]:\n",
    "    print(vect.get_feature_names_out()[i], end=' ')\n",
    "    \n",
    "lookup = dict((v,k) for k,v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all features\n",
    "\n",
    "Just for your reference here is a count and list of all features used in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Total Features: ',len(vect.get_feature_names_out()))\n",
    "print(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments about the results on your first run of this notebook (with defaults)\n",
    "\n",
    "You've probably got something > 0.9 for accuracy. This is pretty good! However, we should ask \"why?\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect correctly/incorrectly classified documents\n",
    "\n",
    "The output in the next cell is quite long and will take a few moments to generate. It will show you wordclouds and a preview of documents for correctly and incorrectly classified documents. The size of words in the wordclouds are based on adding up counts/tf-idf scores of features based on documents related to each cell in the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a counter for each cell in the confusion matrix\n",
    "counter = {}\n",
    "previews = {}\n",
    "for true_target, target_name in enumerate(dataset.target_names):\n",
    "    counter[true_target] = {}\n",
    "    previews[true_target] = {}\n",
    "    for predicted_target, target_name in enumerate(dataset.target_names):\n",
    "        counter[true_target][predicted_target] = {}\n",
    "        previews[true_target][predicted_target] = ''\n",
    "\n",
    "# get doc-term matrix for test docs\n",
    "doc_terms = vect.transform(docs_test)\n",
    "\n",
    "# iterate through all predictions, building the counter and preview of docs\n",
    "# there is a better way to do this, but this will do!\n",
    "for doc_id, prediction in enumerate(clf.predict(doc_terms)):\n",
    "    for k, v in enumerate(doc_terms[doc_id].toarray()[0]):\n",
    "        if v > 0:\n",
    "            if lookup[k] not in counter[y_test[doc_id]][prediction]:\n",
    "                counter[y_test[doc_id]][prediction][lookup[k]] = 0\n",
    "            counter[y_test[doc_id]][prediction][lookup[k]] += v\n",
    "    \n",
    "    previews[y_test[doc_id]][prediction] += get_preview(docs_test, y_test, dataset.target_names, doc_id, max_len=80) + '\\n'\n",
    "\n",
    "# output a wordcloud and preview of docs for each cell of confusion matrix ...\n",
    "for true_target, target_name in enumerate(dataset.target_names):\n",
    "    for predicted_target, target_name in enumerate(dataset.target_names):\n",
    "        if true_target == predicted_target:\n",
    "            print(f'\\nCORRECTLY CLASSIFIED:\\n{dataset.target_names[true_target]}')\n",
    "        else:\n",
    "            print(f'\\n{dataset.target_names[true_target]} INCORRECTLY CLASSIFIED as: {dataset.target_names[predicted_target]}')\n",
    "        print('=================================================================')\n",
    "\n",
    "        wordcloud = WordCloud(background_color=\"white\", width=800, height=400, color_func=lambda *args, **kwargs: \"black\").generate_from_frequencies(counter[true_target][predicted_target])\n",
    "        plt.figure(figsize=(8, 4), dpi= 150)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()        \n",
    "        \n",
    "        print(previews[true_target][predicted_target])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview document and its features\n",
    "\n",
    "Use this cell to preview a document using its index in the test set. You can see the predicted label, its actual label, the full text and the features for this specific document.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Inspect documents that were correct and incorrectly classified. Loaded question: Are there features that are not related to the topics that are making it easier for the model to predict the class? (Hint: this is a \"loaded question\" because the answer is yes!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = 1\n",
    "\n",
    "print('Prediction')\n",
    "print('==========')\n",
    "print(dataset.target_names[clf.predict(vect.transform([docs_test[test_id]]))[0]])\n",
    "print()\n",
    "\n",
    "print(get_preview(docs_test, y_test, dataset.target_names, test_id))\n",
    "\n",
    "print('Features')\n",
    "print('========')\n",
    "for k, v in enumerate(vect.transform([docs_test[test_id]]).toarray()[0]):\n",
    "    if v > 0:\n",
    "        print(v, '\\t', lookup[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer: Why this classification task is perhaps easier than it should be!\n",
    "\n",
    "You will notice features like edu, com, and other portions of email addresses, as well as names that appear in the headers of these newsgroup messages. The information in message headers, as well as footers and quoted material means that the classification task is focused on metadata of the messages rather than the topics themselves. If we are interested in topic-based classifications, features like people's email addresses (who perhaps post often in a newsgroup) are not good signals. To make this more interesting and more challenging for the rest of the lab we need to load the data with just the message text itself. \n",
    "\n",
    "## Classifying based on the \"text\"\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> Return to the cell where you loaded the corpus and set the train/test split. Comment out the line that loads all the data and uncomment the line below it that loads the messages without headers, footers and quotes (note: you can use the keyboard shortcut <code>CTRL + /</code> to comment/uncomment lines). It should look like this after you make the change:\n",
    "    <pre>\n",
    "# this downloads/loads the data\n",
    "# dataset = fetch_20newsgroups(subset='train', categories=cats)\n",
    "dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)</pre>\n",
    "</div>\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 5:</strong> If you haven't already re-run the cell that loads the data and then run the other cells to classify the texts. Note that your accuracy has decreased.     \n",
    "</div>\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 6:</strong> Now we really get into the task of classifying the texts! You can change the settings to try different feature extraction/pre-processing. Pay attention to the way that preprocessing affects the results. Make sure you evaluate your model and inspect the output as you go. Keep notes on what you observe. Once you have improved the accuracy of your model chat to your tutor and also check the Online Text Classification Task on Learn!\n",
    "</div>\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 7:</strong> What documents are the most difficult to classify and why?! Is it to do with specific documents? Or, is it about the classes and the kind of talk that appears in them? Are there certain sub topics that are challenging?\n",
    "</div>\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 8:</strong> Carefully read through the instructions for the Online Text Classification Task on Week 8 of Learn (under the Participation Task heading). You can start on the Online Text Classification Task whenever you like.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digi405",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
